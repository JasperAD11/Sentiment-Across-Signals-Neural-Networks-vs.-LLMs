{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "class SentimentAnalysisModel:\n",
    "    def __init__(self, max_features=10000, max_sequence_length=200, embedding_dim=100):\n",
    "        self.max_features = max_features  # Maximum number of words in the vocabulary\n",
    "        self.max_sequence_length = max_sequence_length  # Maximum length of each text sequence\n",
    "        self.embedding_dim = embedding_dim  # Dimension of word embeddings\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Clean and preprocess text data\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        \n",
    "        # Remove non-alphabetic characters\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords and lemmatize\n",
    "        tokens = [self.lemmatizer.lemmatize(word) for word in tokens if word not in self.stop_words]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def prepare_data(self, texts, labels, test_size=0.2, val_size=0.2):\n",
    "        \"\"\"Prepare and tokenize data for training\"\"\"\n",
    "        # Preprocess all texts\n",
    "        processed_texts = [self.preprocess_text(text) for text in texts]\n",
    "        \n",
    "        # Split data\n",
    "        X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "            processed_texts, labels, test_size=test_size, random_state=42\n",
    "        )\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train_val, y_train_val, test_size=val_size/(1-test_size), random_state=42\n",
    "        )\n",
    "        \n",
    "        # Create and fit tokenizer\n",
    "        self.tokenizer = Tokenizer(num_words=self.max_features, oov_token=\"<OOV>\")\n",
    "        self.tokenizer.fit_on_texts(X_train)\n",
    "        \n",
    "        # Convert texts to sequences\n",
    "        X_train_seq = self.tokenizer.texts_to_sequences(X_train)\n",
    "        X_val_seq = self.tokenizer.texts_to_sequences(X_val)\n",
    "        X_test_seq = self.tokenizer.texts_to_sequences(X_test)\n",
    "        \n",
    "        # Pad sequences\n",
    "        X_train_pad = pad_sequences(X_train_seq, maxlen=self.max_sequence_length, padding='post')\n",
    "        X_val_pad = pad_sequences(X_val_seq, maxlen=self.max_sequence_length, padding='post')\n",
    "        X_test_pad = pad_sequences(X_test_seq, maxlen=self.max_sequence_length, padding='post')\n",
    "        \n",
    "        return (X_train_pad, y_train), (X_val_pad, y_val), (X_test_pad, y_test)\n",
    "    \n",
    "    def build_lstm_model(self, output_dim=1, is_binary=True):\n",
    "        \"\"\"Build an LSTM-based model for sentiment analysis\"\"\"\n",
    "        model = Sequential([\n",
    "            Embedding(self.max_features, self.embedding_dim, input_length=self.max_sequence_length),\n",
    "            Bidirectional(LSTM(128, return_sequences=True)),\n",
    "            Bidirectional(LSTM(64, dropout=0.2)),\n",
    "            Dense(64, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.5),\n",
    "            Dense(output_dim, activation='sigmoid' if is_binary else 'softmax')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy' if is_binary else 'categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "    \n",
    "    def build_cnn_model(self, output_dim=1, is_binary=True):\n",
    "        \"\"\"Build a CNN-based model for sentiment analysis\"\"\"\n",
    "        model = Sequential([\n",
    "            Embedding(self.max_features, self.embedding_dim, input_length=self.max_sequence_length),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            Conv1D(128, 5, activation='relu'),\n",
    "            MaxPooling1D(5),\n",
    "            GlobalMaxPooling1D(),\n",
    "            Dense(128, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.5),\n",
    "            Dense(output_dim, activation='sigmoid' if is_binary else 'softmax')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy' if is_binary else 'categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "    \n",
    "    def train(self, train_data, val_data, epochs=10, batch_size=32):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        X_train, y_train = train_data\n",
    "        X_val, y_val = val_data\n",
    "        \n",
    "        # Setup callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "            ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        self.history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Evaluate model on test data\"\"\"\n",
    "        X_test, y_test = test_data\n",
    "        loss, accuracy = self.model.evaluate(X_test, y_test, verbose=0)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred_prob = self.model.predict(X_test)\n",
    "        \n",
    "        # For binary classification\n",
    "        if y_pred_prob.shape[1] == 1:\n",
    "            y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "            report = classification_report(y_test, y_pred)\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "        else:  # For multi-class\n",
    "            y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "            y_test_classes = np.argmax(y_test, axis=1) if len(y_test.shape) > 1 else y_test\n",
    "            report = classification_report(y_test_classes, y_pred)\n",
    "            cm = confusion_matrix(y_test_classes, y_pred)\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'accuracy': accuracy,\n",
    "            'classification_report': report,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "    \n",
    "    def predict(self, texts):\n",
    "        \"\"\"Make predictions on new text data\"\"\"\n",
    "        # Preprocess texts\n",
    "        processed_texts = [self.preprocess_text(text) for text in texts]\n",
    "        \n",
    "        # Convert to sequences and pad\n",
    "        sequences = self.tokenizer.texts_to_sequences(processed_texts)\n",
    "        padded_sequences = pad_sequences(sequences, maxlen=self.max_sequence_length, padding='post')\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = self.model.predict(padded_sequences)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Plot accuracy\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.history.history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(self.history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot loss\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.history.history['loss'], label='Train Loss')\n",
    "        plt.plot(self.history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_confusion_matrix(self, cm, classes=None):\n",
    "        \"\"\"Plot confusion matrix\"\"\"\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=classes, yticklabels=classes)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def save_model(self, model_path):\n",
    "        \"\"\"Save model and tokenizer\"\"\"\n",
    "        # Save model\n",
    "        self.model.save(model_path)\n",
    "        \n",
    "        # Save tokenizer\n",
    "        import pickle\n",
    "        with open(f\"{os.path.splitext(model_path)[0]}_tokenizer.pickle\", 'wb') as handle:\n",
    "            pickle.dump(self.tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(cls, model_path):\n",
    "        \"\"\"Load a saved model and tokenizer\"\"\"\n",
    "        # Create new instance\n",
    "        instance = cls()\n",
    "        \n",
    "        # Load model\n",
    "        instance.model = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        # Load tokenizer\n",
    "        import pickle\n",
    "        with open(f\"{os.path.splitext(model_path)[0]}_tokenizer.pickle\", 'rb') as handle:\n",
    "            instance.tokenizer = pickle.load(handle)\n",
    "        \n",
    "        return instance\n",
    "\n",
    "\n",
    "# Example usage for binary sentiment classification (IMDb dataset)\n",
    "def run_binary_sentiment_example():\n",
    "    # Load IMDb dataset\n",
    "    from tensorflow.keras.datasets import imdb\n",
    "    \n",
    "    # Load data\n",
    "    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)\n",
    "    \n",
    "    # Convert back to text (IMDb dataset loads as sequences already)\n",
    "    word_index = imdb.get_word_index()\n",
    "    # Create a reverse mapping\n",
    "    reverse_word_index = {value: key for key, value in word_index.items()}\n",
    "    \n",
    "    def sequence_to_text(sequence):\n",
    "        # Add 3 because 0, 1, and 2 are reserved indices\n",
    "        return ' '.join([reverse_word_index.get(i - 3, '?') for i in sequence if i > 3])\n",
    "    \n",
    "    X_train_texts = [sequence_to_text(seq) for seq in X_train]\n",
    "    X_test_texts = [sequence_to_text(seq) for seq in X_test]\n",
    "    \n",
    "    # Create and train LSTM model\n",
    "    sentiment_model = SentimentAnalysisModel(max_features=10000)\n",
    "    (train_data, val_data, test_data) = sentiment_model.prepare_data(\n",
    "        X_train_texts + X_test_texts[:1000],  # Using part of test set to have more training data\n",
    "        np.concatenate([y_train, y_test[:1000]])\n",
    "    )\n",
    "    \n",
    "    # Build LSTM model\n",
    "    sentiment_model.build_lstm_model()\n",
    "    print(sentiment_model.model.summary())\n",
    "    \n",
    "    # Train model\n",
    "    history = sentiment_model.train(train_data, val_data, epochs=5)\n",
    "    \n",
    "    # Evaluate\n",
    "    results = sentiment_model.evaluate(test_data)\n",
    "    print(\"Evaluation Results:\")\n",
    "    print(f\"Loss: {results['loss']:.4f}\")\n",
    "    print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(results['classification_report'])\n",
    "    \n",
    "    # Plot results\n",
    "    sentiment_model.plot_training_history()\n",
    "    sentiment_model.plot_confusion_matrix(results['confusion_matrix'], ['Negative', 'Positive'])\n",
    "    \n",
    "    # Save model\n",
    "    sentiment_model.save_model('imdb_lstm_model.h5')\n",
    "    \n",
    "    # Try CNN model for comparison\n",
    "    cnn_model = SentimentAnalysisModel(max_features=10000)\n",
    "    # Reuse the prepared data\n",
    "    cnn_model.tokenizer = sentiment_model.tokenizer\n",
    "    cnn_model.build_cnn_model()\n",
    "    print(cnn_model.model.summary())\n",
    "    \n",
    "    # Train CNN model\n",
    "    cnn_history = cnn_model.train(train_data, val_data, epochs=5)\n",
    "    \n",
    "    # Evaluate CNN\n",
    "    cnn_results = cnn_model.evaluate(test_data)\n",
    "    print(\"\\nCNN Model Evaluation Results:\")\n",
    "    print(f\"Loss: {cnn_results['loss']:.4f}\")\n",
    "    print(f\"Accuracy: {cnn_results['accuracy']:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(cnn_results['classification_report'])\n",
    "    \n",
    "    # Plot CNN results\n",
    "    cnn_model.plot_training_history()\n",
    "    cnn_model.plot_confusion_matrix(cnn_results['confusion_matrix'], ['Negative', 'Positive'])\n",
    "    \n",
    "    # Save CNN model\n",
    "    cnn_model.save_model('imdb_cnn_model.h5')\n",
    "\n",
    "\n",
    "# Example usage for multi-class emotion detection (assuming GoEmotions dataset)\n",
    "def run_multiclass_emotion_example():\n",
    "    # This is a placeholder - you would need to download the GoEmotions dataset\n",
    "    # https://github.com/google-research/google-research/tree/master/goemotions\n",
    "    \n",
    "    # Example function to load GoEmotions dataset - adapt this to actual data format\n",
    "    def load_goemotions(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Assuming format with text column and multiple emotion columns\n",
    "        texts = df['text'].values\n",
    "        \n",
    "        # Extract emotion columns, assuming one-hot encoded format\n",
    "        emotion_columns = ['joy', 'sadness', 'anger', 'fear', 'surprise', 'disgust']\n",
    "        emotions = df[emotion_columns].values\n",
    "        \n",
    "        return texts, emotions\n",
    "    \n",
    "    try:\n",
    "        # Try to load the dataset - replace with actual path\n",
    "        texts, emotions = load_goemotions('goemotions.csv')\n",
    "        \n",
    "        # Create and train model for multi-class classification\n",
    "        emotion_model = SentimentAnalysisModel(max_features=15000)\n",
    "        (train_data, val_data, test_data) = emotion_model.prepare_data(texts, emotions)\n",
    "        \n",
    "        # Number of emotion classes\n",
    "        num_emotions = emotions.shape[1]\n",
    "        \n",
    "        # Build LSTM model for multi-class\n",
    "        emotion_model.build_lstm_model(output_dim=num_emotions, is_binary=False)\n",
    "        print(emotion_model.model.summary())\n",
    "        \n",
    "        # Train model\n",
    "        history = emotion_model.train(train_data, val_data, epochs=10)\n",
    "        \n",
    "        # Evaluate\n",
    "        results = emotion_model.evaluate(test_data)\n",
    "        print(\"Emotion Detection Results:\")\n",
    "        print(f\"Loss: {results['loss']:.4f}\")\n",
    "        print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(results['classification_report'])\n",
    "        \n",
    "        # Plot results\n",
    "        emotion_model.plot_training_history()\n",
    "        emotion_labels = ['joy', 'sadness', 'anger', 'fear', 'surprise', 'disgust']\n",
    "        emotion_model.plot_confusion_matrix(results['confusion_matrix'], emotion_labels)\n",
    "        \n",
    "        # Save model\n",
    "        emotion_model.save_model('emotion_lstm_model.h5')\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"GoEmotions dataset not found. Please download it and specify the correct path.\")\n",
    "        print(\"You can find the dataset at: https://github.com/google-research/google-research/tree/master/goemotions\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running binary sentiment analysis example (IMDb)...\")\n",
    "    run_binary_sentiment_example()\n",
    "    \n",
    "    print(\"\\nTo run the multi-class emotion example, first download the GoEmotions dataset.\")\n",
    "    print(\"Then uncomment the call to run_multiclass_emotion_example() in the main block.\")\n",
    "    # run_multiclass_emotion_example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
