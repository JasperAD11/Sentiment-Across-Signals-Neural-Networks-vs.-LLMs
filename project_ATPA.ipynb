{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1xBPq0Mm5Knt",
        "Gbajqz7JY0Bb",
        "c5GgOPl07z5w",
        "2fTFHeo-5qgK"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNBwSt5TB+O6kCgegO1HtSc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JasperAD11/Sentiment-Across-Signals-Neural-Networks-vs.-LLMs/blob/main/project_ATPA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TO DO**:\n",
        "  1. Compute results for other models (change parameters) and compare.\n",
        "  2. Code doesn't run for the second model\n",
        "\n",
        "**Doubts**:\n",
        "  1. what if the same word appears 2x in the same review/newswire, is that explicit in the tensor?"
      ],
      "metadata": {
        "id": "YwcdhsMlEhLz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project: Part 1"
      ],
      "metadata": {
        "id": "253PasrCyWx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions and libraries needed"
      ],
      "metadata": {
        "id": "cGHnFVplieGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing libraries"
      ],
      "metadata": {
        "id": "tQo3udH5y7eW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os, pathlib, shutil, random\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Embedding, TextVectorization\n"
      ],
      "metadata": {
        "id": "LSWHjvHfTxHI"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a fixed random seed value\n",
        "SEED = 42\n",
        "\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ],
      "metadata": {
        "id": "YkpF8X_JT2xl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorizing text inputs"
      ],
      "metadata": {
        "id": "lr2xnVMXlcBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing text\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"tf_idf\",\n",
        ")"
      ],
      "metadata": {
        "id": "e7yI10dcks_q"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-trained embedding layer - GloVe"
      ],
      "metadata": {
        "id": "K56CFuROlhCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only need to run the following once\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "id": "2mnQYcd0wjdY",
        "outputId": "90de7373-49bc-4541-a32b-f8c536bcbfed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-29 14:44:36--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-04-29 14:44:37--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-04-29 14:44:37--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  5.11MB/s    in 2m 44s  \n",
            "\n",
            "2025-04-29 14:47:23 (5.00 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Only need to run the following once\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "id": "Nf-0RRwswpcM",
        "outputId": "14aac7fa-7a86-4753-df22-9c4af627c9b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading pre-trained word embedding (GloVe)   PAGE 333 BOOK\n",
        "path_to_glove_file = \"glove.6B.100d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "  for line in f:\n",
        "    word, coefs = line.split(maxsplit=1)\n",
        "    coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "    embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ],
      "metadata": {
        "id": "6fDeMZqPiiQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "max_tokens = 20000\n",
        "\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "\n",
        "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "  if i < max_tokens:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "ev40BZgsj_E2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = layers.Embedding(\n",
        "    max_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        "    mask_zero=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "Lvb8VHK8oeXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1: Binary sentiment classification"
      ],
      "metadata": {
        "id": "ZDVyLJnxWJR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset\n"
      ],
      "metadata": {
        "id": "1xBPq0Mm5Knt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only need to run the following once\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "AhjRy3uztecE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "\n",
        "for category in (\"neg\", \"pos\"):\n",
        "  os.makedirs(val_dir / category)\n",
        "  files = os.listdir(train_dir / category)\n",
        "  random.Random(1337).shuffle(files)\n",
        "  num_val_samples = int(0.2 * len(files))\n",
        "  val_files = files[-num_val_samples:]\n",
        "  for fname in val_files:\n",
        "    shutil.move(train_dir / category / fname,\n",
        "                val_dir / category / fname)"
      ],
      "metadata": {
        "id": "-dqyV_iJ1GQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size)\n",
        "\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size)\n",
        "\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size)"
      ],
      "metadata": {
        "id": "RdwtePpN2Q5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "7o97fInk3WTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "# keeping only the 10000 most frequent word"
      ],
      "metadata": {
        "id": "3X-h0C3izS7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Turning the list of words(numbers) into tensors -\n",
        "IS THIS STILL IMPORTANT?\n"
      ],
      "metadata": {
        "id": "c5GgOPl07z5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The underlying idea is that we want to have a matrix with 10000 columns (one for each of the top10000 words) and a row for each review.\n"
      ],
      "metadata": {
        "id": "TvHMEmCF9eKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_sequences(sequences, dimension=10000):     # dimension=10000 because there are 10000 different words\n",
        "  results = np.zeros((len(sequences), dimension))      # starting with a tensor of all 0.(s)\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    for j in sequence:                                # j is each index inside a nested list in data = each word in a review\n",
        "      results[i, j] = 1.                            # inside a tensor inside \"results\", 1. is that word appers\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "# After the transformation above, each review is now a RANK-1 TENSOR with 10000 dimensions.\n",
        "# After the transformation above, each train/test_data is now a RANK-2 TENSOR with 10000 dimensions.\n",
        "\n",
        "# x_train[0]"
      ],
      "metadata": {
        "id": "27Z7Jd8367Qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, we need to vectorize labels. For that, we do the following:"
      ],
      "metadata": {
        "id": "4uENQK0J_NSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.asarray(train_labels).astype(\"float32\")\n",
        "y_test = np.asarray(test_labels).astype(\"float32\")"
      ],
      "metadata": {
        "id": "iCo3sN5-_BOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN model"
      ],
      "metadata": {
        "id": "ha9ekmfat7Tq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing the architecture of the model:"
      ],
      "metadata": {
        "id": "Mv97LzFU2fS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1_cnn = keras.Sequential([\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model1_cnn.compile(\n",
        "    optimizer='adam',               # better than rmsprop\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "mz-uM0-yt_eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting train into partial_train and validation\n",
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "metadata": {
        "id": "MMctVXHM130L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now train the model."
      ],
      "metadata": {
        "id": "_-H4Y-tq2PlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "history = model1_cnn.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks=[early_stopping])\n"
      ],
      "metadata": {
        "id": "-R0dZP-J2O_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1_cnn.summary()"
      ],
      "metadata": {
        "id": "PnmOWJpkb0DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# History objects have a member 'history' that is a dict\n",
        "history_dict = history.history\n",
        "\n",
        "history_dict.keys()\n",
        "# history_dict.values()"
      ],
      "metadata": {
        "id": "JkXtvrCa3wfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plotting:"
      ],
      "metadata": {
        "id": "2fTFHeo-5qgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss\n",
        "history_dict = history.history\n",
        "loss_values = history_dict[\"loss\"]\n",
        "val_loss_values = history_dict[\"val_loss\"]\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HbIcLsqa5qBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "plt.clf()\n",
        "acc = history_dict[\"accuracy\"]\n",
        "val_acc = history_dict[\"val_accuracy\"]\n",
        "plt.plot(epochs, acc, \"bo\", label=\"Training acc\")\n",
        "plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6gKiQIBp6AdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM model"
      ],
      "metadata": {
        "id": "BJw5hU5WcIqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "max_features = 10000  # Size of vocabulary\n",
        "maxlen = 500  # Max sequence length\n",
        "embedding_dim = 128  # Embedding dimension\n",
        "\n",
        "# Create the LSTM model\n",
        "model_lstm = keras.Sequential([\n",
        "    # Embedding layer converts integer indices to dense vectors\n",
        "    layers.Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=maxlen),\n",
        "\n",
        "    # Bidirectional LSTM to process sequence in both directions\n",
        "    layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n",
        "\n",
        "    # Second LSTM layer for deeper representation\n",
        "    layers.Bidirectional(layers.LSTM(32)),\n",
        "\n",
        "    # Dropout for regularization\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "    # Dense hidden layer\n",
        "    layers.Dense(64, activation='relu'),\n",
        "\n",
        "    # Output layer for binary classification\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_lstm.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "AMWmFypvcNBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting train into partial_train and validation\n",
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "metadata": {
        "id": "WMMF-tdadPlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_lstm.fit(partial_x_train,                       # history is an 'History' object\n",
        "                          partial_y_train,\n",
        "                          epochs=20,\n",
        "                          batch_size=32,\n",
        "                          validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "rNFlT4WGdXpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.summary()"
      ],
      "metadata": {
        "id": "PJrpO6t_dZmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# History objects have a member 'history' that is a dict\n",
        "history_dict = history.history\n",
        "\n",
        "history_dict.keys()\n",
        "# history_dict.values()"
      ],
      "metadata": {
        "id": "KhC0V0xfdqEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plotting:"
      ],
      "metadata": {
        "id": "JpJaWaOqeTZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss\n",
        "history_dict = history.history\n",
        "loss_values = history_dict[\"loss\"]\n",
        "val_loss_values = history_dict[\"val_loss\"]\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UyFDFh7_eVWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "plt.clf()\n",
        "acc = history_dict[\"accuracy\"]\n",
        "val_acc = history_dict[\"val_accuracy\"]\n",
        "plt.plot(epochs, acc, \"bo\", label=\"Training acc\")\n",
        "plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V3js4lg2edsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final model"
      ],
      "metadata": {
        "id": "XsxCCIe4eLUN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retraining the model in all the train data and evaluation in test data.\n",
        "After that we are computing the results."
      ],
      "metadata": {
        "id": "4h7LCl3387b2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model_lstm.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=32,\n",
        "    epochs=10,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=2),\n",
        "        keras.callbacks.ModelCheckpoint('best_lstm_model.h5', save_best_only=True)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "g__PxA31eChT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer=,\n",
        "              loss=,\n",
        "              metrics=)\n",
        "\n",
        "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
        "\n",
        "results = model.evaluate(x_test, y_test)\n",
        "\n",
        "results"
      ],
      "metadata": {
        "id": "8rWiEv_R8gWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making predictions:"
      ],
      "metadata": {
        "id": "XKHebbUuFrgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(x_test)"
      ],
      "metadata": {
        "id": "ZkAwATljFvFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Can we improve the model? How?"
      ],
      "metadata": {
        "id": "O5nswqwWGII7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We experiment different architecture:\n",
        "1. Add another layer / Take out one layer.\n",
        "2. Increse/Decrease perceptrons inside layers (usualy factors of 8: 32, 64, 128).\n",
        "3. Try 'mse' for loss function.\n",
        "4. Try 'tanh' for activation function."
      ],
      "metadata": {
        "id": "Xmuc3FDbGQMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model that uses pre trained embedding (book)\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = embedding_layer(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "loss=\"binary_crossentropy\",\n",
        "metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "callbacks = [\n",
        "keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",\n",
        "save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10,\n",
        "callbacks=callbacks)\n",
        "model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "_TDnqDhXhtZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Model 2: Multi-class emotion detection"
      ],
      "metadata": {
        "id": "USMhtAQieuXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import reuters   # Dataset with news labeled with a topic"
      ],
      "metadata": {
        "id": "Bt8Vp8V4kVUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "9paB0SWChMi0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(using reuters dataset inside tensorflow.keras.datasets)"
      ],
      "metadata": {
        "id": "YQd8r27OfR2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_1= pd.read_csv(\"goemotions_1.csv\")\n",
        "df_2= pd.read_csv(\"goemotions_2.csv\")\n",
        "df_3= pd.read_csv(\"goemotions_3.csv\")"
      ],
      "metadata": {
        "id": "O8EYGGl0smFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_data, train_labels),(test_data, test_labels) = reuters.load_data(num_words = 100000)\n",
        "# again we are only interested in the TOP10000 words"
      ],
      "metadata": {
        "id": "xSwdDbHafc7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### not necessary"
      ],
      "metadata": {
        "id": "C8bzO2dsR5Fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "metadata": {
        "id": "Z4HqHmWOgYXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_data)"
      ],
      "metadata": {
        "id": "qWxvJq2Pgb1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_labels)"
      ],
      "metadata": {
        "id": "EeI_QU-oSBUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the format of the data is the same as in the previous case, where each piece of news is encode into a list of integers, being each of them correspondent to a word."
      ],
      "metadata": {
        "id": "24IDvjXrgjon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extra: decoding back to word (same as in imdb case)"
      ],
      "metadata": {
        "id": "LmL1OeTKhPAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = reuters.get_word_index()    # dict that maps each word to its code number\n",
        "\n",
        "reverse_word_index = dict(            # dict inverting value and key (number to word)\n",
        "    [(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "# Decoding train_data[0]\n",
        "decoded_newswire = \" \".join(\n",
        "    [reverse_word_index.get(i - 3, \"?\") for i in train_data[0]])\n",
        "\n",
        "decoded_newswire"
      ],
      "metadata": {
        "id": "U6hfxk-qhY_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Turning lists into tensors (same code as before)"
      ],
      "metadata": {
        "id": "73oC4_Hoh2jG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dont understand this error\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "metadata": {
        "id": "yzikqK92h8sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing labels (diferent than in the previous case because there are more than 2 labels)\n",
        "\n",
        "def to_one_hot(labels, dimension=46):\n",
        "  results = np.zeros((len(labels), dimension))\n",
        "  for i, label in enumerate(labels):\n",
        "    results[i, label] = 1.\n",
        "  return results\n",
        "\n",
        "# this yields a matrix with 46 dimensions(columns),\n",
        "# where all values in a row are 0 except in the column with the number of the correct label\n",
        "\n",
        "y_train = to_one_hot(train_labels)\n",
        "y_test = to_one_hot(test_labels)"
      ],
      "metadata": {
        "id": "9p6Q5y7Oije1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# There is all this built in way to do it\n",
        "# from tensorflow.keras.utils import to_categorical\n",
        "# y_train = to_categorical(train_labels)\n",
        "# y_test = to_categorical(test_labels)"
      ],
      "metadata": {
        "id": "Eh4aH4sdj5NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN model"
      ],
      "metadata": {
        "id": "iFVwVyAtkbaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 10000   # Define maximum number of words to consider in the vocabulary\n",
        "maxlen = 500           # Define sequence length (truncate or pad sequences to this length)\n",
        "embedding_dim = 128    # Embedding dimension\n",
        "\n",
        "# Create the model\n",
        "model_cnn = keras.Sequential([\n",
        "    # Embedding layer to convert word indices to dense vectors\n",
        "    layers.Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=maxlen),\n",
        "\n",
        "    # 1D CNN layers for extracting n-gram features\n",
        "    layers.Conv1D(filters=128, kernel_size=5, activation='relu', padding='same'),\n",
        "    layers.MaxPooling1D(pool_size=5),\n",
        "    layers.Conv1D(filters=128, kernel_size=5, activation='relu', padding='same'),\n",
        "    layers.MaxPooling1D(pool_size=5),\n",
        "    layers.Conv1D(filters=128, kernel_size=5, activation='relu', padding='same'),\n",
        "    layers.GlobalMaxPooling1D(),\n",
        "\n",
        "    # Dense layers for classification\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),  # Add dropout to reduce overfitting\n",
        "    layers.Dense(1, activation='sigmoid')  # Binary classification output\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_cnn.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# model.compile(optimizer=\"rmsprop\",            # choosing optimizer\n",
        "#               loss=\"binary_crossentropy\",     # choosing loss function\n",
        "#               metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "29hR64WZkyAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compute validation datasets."
      ],
      "metadata": {
        "id": "DtS6HaDxlxEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting train into partial_train and validation\n",
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "metadata": {
        "id": "AGA-zoVKl03Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now train the model"
      ],
      "metadata": {
        "id": "w2odJi44l7LU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_cnn.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "JzFKvXrll-SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plotting"
      ],
      "metadata": {
        "id": "W9jiMlIKoPKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss\n",
        "history_dict = history.history\n",
        "loss_values = history_dict[\"loss\"]\n",
        "val_loss_values = history_dict[\"val_loss\"]\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8UZxX802oOdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "plt.clf()\n",
        "acc = history_dict[\"accuracy\"]\n",
        "val_acc = history_dict[\"val_accuracy\"]\n",
        "plt.plot(epochs, acc, \"bo\", label=\"Training acc\")\n",
        "plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bX7gpa46oU1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer model"
      ],
      "metadata": {
        "id": "xk2W40LxlL75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "max_features = 10000  # Size of vocabulary\n",
        "maxlen = 500  # Max sequence length\n",
        "embedding_dim = 128  # Embedding dimension\n",
        "\n",
        "# Transformer block\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Multi-head self-attention\n",
        "    attention_output = layers.MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=head_size\n",
        "    )(inputs, inputs)\n",
        "    attention_output = layers.Dropout(dropout)(attention_output)\n",
        "    attention_output = layers.LayerNormalization(epsilon=1e-6)(\n",
        "        inputs + attention_output\n",
        "    )\n",
        "\n",
        "    # Feed-forward network\n",
        "    ffn_output = layers.Dense(ff_dim, activation=\"relu\")(attention_output)\n",
        "    ffn_output = layers.Dense(inputs.shape[-1])(ffn_output)\n",
        "    ffn_output = layers.Dropout(dropout)(ffn_output)\n",
        "\n",
        "    # Second residual connection and layer normalization\n",
        "    return layers.LayerNormalization(epsilon=1e-6)(\n",
        "        attention_output + ffn_output\n",
        "    )\n",
        "\n",
        "# Define input\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "\n",
        "# Embedding layer\n",
        "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
        "\n",
        "# Add positional encoding\n",
        "positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "position_embedding = layers.Embedding(\n",
        "    input_dim=maxlen, output_dim=embedding_dim\n",
        ")(positions)\n",
        "x = x + position_embedding\n",
        "\n",
        "# Apply dropout\n",
        "x = layers.Dropout(0.1)(x)\n",
        "\n",
        "# Apply transformer blocks\n",
        "transformer_blocks = 2\n",
        "for _ in range(transformer_blocks):\n",
        "    x = transformer_encoder(x, head_size=64, num_heads=2, ff_dim=128, dropout=0.1)\n",
        "\n",
        "# Pool across sequence dimension and apply classification layers\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "# Build the model\n",
        "model_transf = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model_transf.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "metadata": {
        "id": "8AVEubDAlSBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model_transf.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=32,\n",
        "    epochs=5,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=2),\n",
        "        keras.callbacks.ModelCheckpoint('best_transformer_model.h5', save_best_only=True)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "J9xUS4b-mc1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_transf.summary()"
      ],
      "metadata": {
        "id": "23-camCroaF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test set\n",
        "test_loss, test_acc = model_transf.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc:.4f}')\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "KaFACX4Uon8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final model"
      ],
      "metadata": {
        "id": "l6SsxOwmVABX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# what does this do?\n",
        "import copy\n",
        "test_labels_copy = copy.copy(test_labels)\n",
        "np.random.shuffle(test_labels_copy)\n",
        "hits_array = np.array(test_labels) == np.array(test_labels_copy)\n",
        "hits_array.mean()"
      ],
      "metadata": {
        "id": "Gm8gWKZ0VhEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making predictions:"
      ],
      "metadata": {
        "id": "EKON4S1NV266"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(x_test)"
      ],
      "metadata": {
        "id": "Vb_p_LU4VxSP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}