{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JasperAD11/Sentiment-Across-Signals-Neural-Networks-vs.-LLMs/blob/main/notebook_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AeNj0txdNof"
      },
      "source": [
        "# Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWdQVt36dJw3"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sdv90RuERAUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7120bef9-da41-4a9e-838a-3bb7e0d62337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.11/dist-packages (20240930)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "! pip install -U openai-whisper\n",
        "import whisper\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, initializers\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import TextVectorization, Input, Embedding, LSTM, Dropout, Dense\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdOsEDOmS2GP"
      },
      "source": [
        "## Binary model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRRWrFkuXuPL"
      },
      "source": [
        "### Dataset binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYev4KopQqyO",
        "outputId": "bc497fe0-b306-41a4-fa03-1ee14efa1c8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  58.2M      0  0:00:01  0:00:01 --:--:-- 58.1M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xzf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVgVZipkQtOG",
        "outputId": "b007cd56-c123-4612-8972-4b1483a6b5fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# Directory path\n",
        "dataset_dir = \"aclImdb\"\n",
        "\n",
        "# Remove unsup data (not labeled)\n",
        "shutil.rmtree(os.path.join(dataset_dir, 'train', 'unsup'))\n",
        "\n",
        "# Load training and test sets\n",
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    os.path.join(dataset_dir, \"train\"),\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    os.path.join(dataset_dir, \"train\"),\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    os.path.join(dataset_dir, \"test\"),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# To train the Final Model\n",
        "full_train_ds = train_ds.concatenate(val_ds).shuffle(10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PRCY6qy3Qx4h"
      },
      "outputs": [],
      "source": [
        "max_vocab = 20000\n",
        "sequence_len = 300\n",
        "\n",
        "vectorizer = TextVectorization(\n",
        "    max_tokens=max_vocab,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_len\n",
        ")\n",
        "\n",
        "# Adapt vectorizer on training data\n",
        "text_only_train = train_ds.map(lambda x, y: x)\n",
        "vectorizer.adapt(text_only_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert datasets to NumPy arrays or tensors\n",
        "def vectorize_dataset(ds):\n",
        "    return ds.map(lambda x, y: (vectorizer(x), y)).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "train_ds = vectorize_dataset(train_ds)\n",
        "val_ds = vectorize_dataset(val_ds)\n",
        "test_ds = vectorize_dataset(test_ds)\n",
        "full_train_ds = vectorize_dataset(full_train_ds)\n"
      ],
      "metadata": {
        "id": "FpQSe_LYGOoj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keKdu_WkjTEn"
      },
      "source": [
        "### Final Binary Model (model 2 from notebook1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bQvcBwb_pV4F"
      },
      "outputs": [],
      "source": [
        "model_binary = keras.Sequential([\n",
        "    layers.Embedding(input_dim=max_vocab, output_dim=128),\n",
        "    layers.GlobalAveragePooling1D(),\n",
        "\n",
        "    # Dense layer 1\n",
        "    layers.Dense(8, activation='relu'),\n",
        "\n",
        "    # Dense layer 2\n",
        "    layers.Dense(8, activation='relu'),\n",
        "\n",
        "    # Dense layer 3 (Output)\n",
        "    layers.Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "model_binary.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_xoBVZlXjrvu",
        "outputId": "b6e1521e-8789-4046-c146-f251f27152f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FailedPreconditionError",
          "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-7-03486c18c34c>\", line 1, in <cell line: 0>\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_28642]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-03486c18c34c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model_binary.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mfull_train_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     callbacks = [\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-7-03486c18c34c>\", line 1, in <cell line: 0>\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_28642]"
          ]
        }
      ],
      "source": [
        "history = model_binary.fit(\n",
        "    full_train_ds,\n",
        "    validation_data = test_ds,\n",
        "    epochs=30,\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_AUC', patience=10, restore_best_weights=True, mode='max'),\n",
        "        ModelCheckpoint('best_model_binary.h5', monitor='val_accuracy', save_best_only=True, mode='max')]\n",
        ")\n",
        "\n",
        "model_binary.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUUi8qM-X3_J"
      },
      "source": [
        "## Multi-class model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj8HI6SJYBNC"
      },
      "source": [
        "### Dataset multi-class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IK9dIE9fYEmM"
      },
      "outputs": [],
      "source": [
        "# Unzip to a folder\n",
        "!unzip emotions-goemotions.zip -d emotions_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjCs-rBbh60e"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('emotions_data/goemotions/data/full_dataset/goemotions_1.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8AML6u7iWus"
      },
      "outputs": [],
      "source": [
        "dataset.drop(columns=[\"id\",\"author\",\"subreddit\",\"link_id\",\"parent_id\",\"created_utc\",\"rater_id\",\"example_very_unclear\"], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFGgNap5iaYi"
      },
      "outputs": [],
      "source": [
        "# vectorizer.adapt(dataset['text'].values)\n",
        "\n",
        "X = vectorizer(dataset['text'].values)\n",
        "\n",
        "y = dataset.drop(columns=['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JjuYQ34i2Sf"
      },
      "outputs": [],
      "source": [
        "X_numpy = X.numpy() if isinstance(X, tf.Tensor) else X\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(X_numpy, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Second split: Take 20% of training for validation (16% of original)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_full,\n",
        "    y_train_full,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VLjMbEwCEGJ"
      },
      "source": [
        "### Final Multi-class Model (model 5 from notebook1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLh1oPPSCnt6"
      },
      "outputs": [],
      "source": [
        "model_multi_class = keras.Sequential([\n",
        "    layers.Embedding(input_dim=max_vocab, output_dim=128),\n",
        "    layers.GlobalAveragePooling1D(),\n",
        "\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Dense(28, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_multi_class.compile(optimizer='adam',\n",
        "               loss='binary_crossentropy',\n",
        "               metrics=['AUC'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_multi_class.fit(X_train_full,\n",
        "                                y_train_full,\n",
        "                                epochs=50,\n",
        "                                batch_size=32,\n",
        "                                # validation_data=(X_val, y_val),\n",
        "                                validation_data=(X_train_full, y_train_full),\n",
        "                                callbacks = [\n",
        "                                    EarlyStopping(monitor='val_AUC', patience=10, restore_best_weights=True),\n",
        "                                    ModelCheckpoint('best_model_multi.h5', monitor='val_AUC', save_best_only=True, mode='max')]\n",
        ")\n",
        "model_multi_class.summary()"
      ],
      "metadata": {
        "id": "fH0-eKE9kBR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhkhEzOfNwNz"
      },
      "source": [
        "## Merging **Binary** and **Multi class** models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_ensemble_model(sentiment_model_path, emotion_model_path):\n",
        "    # Load the models\n",
        "    sentiment_model = load_model(sentiment_model_path)\n",
        "    emotion_model = load_model(emotion_model_path)\n",
        "\n",
        "    # Freeze the models to prevent training\n",
        "    sentiment_model.trainable = False\n",
        "    emotion_model.trainable = False\n",
        "\n",
        "    # Define new input layers\n",
        "    sentiment_input = Input(shape=sentiment_model.input_shape[1:], name=\"sentiment_input\")\n",
        "    emotion_input = Input(shape=emotion_model.input_shape[1:], name=\"emotion_input\")\n",
        "\n",
        "    # Pass the inputs through the respective models\n",
        "    sentiment_output = sentiment_model(sentiment_input)\n",
        "    emotion_output = emotion_model(emotion_input)\n",
        "\n",
        "    # Create the joint model\n",
        "    joint_model = Model(\n",
        "        inputs=[sentiment_input, emotion_input],\n",
        "        outputs=[sentiment_output, emotion_output]\n",
        "    )\n",
        "\n",
        "    return joint_model"
      ],
      "metadata": {
        "id": "scVXHaTxdGGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joint_model=create_ensemble_model('best_model_binary.h5', 'best_model_multi.h5')\n",
        "\n",
        "joint_model.summary()"
      ],
      "metadata": {
        "id": "_jSofNKUdL93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define emotion labels (adjust to your actual labels)\n",
        "emotion_labels = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity',\n",
        "                  'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear',\n",
        "                  'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
        "                  'remorse', 'sadness', 'surprise', 'neutral']\n",
        "\n",
        "def predict_ensemble_model(model, texts, vectorizer, emotion_labels=emotion_labels, max_length=300, neutral_threshold=0.3, emotion_threshold=0.15):\n",
        "    # Tokenize and pad the input texts\n",
        "    input = vectorizer(texts)\n",
        "\n",
        "    # Make predictions with the joint model\n",
        "    predictions = model.predict({\n",
        "        'sentiment_input': input,\n",
        "        'emotion_input': input\n",
        "    })\n",
        "\n",
        "    # BINARY\n",
        "    # Get the sentiment prediction\n",
        "    sentiment_prediction = predictions[0]\n",
        "\n",
        "    # Convert sentiment prediction to 'positive' or 'negative' based on threshold of 0.5\n",
        "    sentiment_label = \"positive\" if sentiment_prediction[0] > 0.5 else \"negative\"\n",
        "\n",
        "\n",
        "    # MULTI-CLASS\n",
        "    # Get emotion predictions\n",
        "    emotion_predictions = predictions[1]\n",
        "\n",
        "    # Define emotion labels (adjust to your actual labels)\n",
        "    emotion_labels = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity',\n",
        "                      'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear',\n",
        "                      'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
        "                      'remorse', 'sadness', 'surprise', 'neutral']\n",
        "\n",
        "    # Map the emotion predictions to the emotion labels\n",
        "    emotion_results = {emotion_labels[i]: emotion_predictions[0][i] for i in range(len(emotion_labels))}\n",
        "\n",
        "\n",
        "    # Check if 'neutral' emotion has score > neutral_threshold\n",
        "    if emotion_results.get('neutral', 0) >= neutral_threshold:\n",
        "        # If neutral is above the threshold, only return \"neutral\"\n",
        "        return {\n",
        "            'sentiment': sentiment_label,\n",
        "            'emotion': ['neutral']\n",
        "        }\n",
        "\n",
        "    # Filter emotions: return all emotions > emotion_threshold, excluding 'neutral'\n",
        "    filtered_emotions = {emotion: score for emotion, score in emotion_results.items() if score > emotion_threshold and emotion != 'neutral'}\n",
        "\n",
        "    # If no emotions are above the threshold, return only the emotion with the highest score, excluding 'neutral'\n",
        "    if not filtered_emotions:\n",
        "        max_emotion = max((emotion_results[key], key) for key in emotion_results if key != 'neutral')\n",
        "        filtered_emotions = {max_emotion[1]: max_emotion[0]}\n",
        "\n",
        "    # Return the predictions\n",
        "    return {\n",
        "        'sentiment': sentiment_label,  # Sentiment prediction as 'positive' or 'negative'\n",
        "        'emotion': list(filtered_emotions.keys())  # List of emotions above threshold or best emotion\n",
        "    }\n"
      ],
      "metadata": {
        "id": "l-yupvgcdbxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_ensemble_model(joint_model, [\"I am so excited!\"], vectorizer)"
      ],
      "metadata": {
        "id": "UO_zLxj1gjZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Whisper"
      ],
      "metadata": {
        "id": "tXQWQyaQ0eyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Audios.zip -d audios_data"
      ],
      "metadata": {
        "id": "-pY008ZFRMhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Whisper model\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Folder where your .mp4 files are located\n",
        "folder = \"/content/audios_data/audios\"\n",
        "\n",
        "# Transcribe each file\n",
        "transcripts = []\n",
        "for filename in os.listdir(folder):\n",
        "    if filename.endswith(\".mp4\"):\n",
        "        path = os.path.join(folder, filename)\n",
        "        print(f\"Transcribing: {filename}\")\n",
        "        result = model.transcribe(path, fp16=False)\n",
        "        transcripts.append({\n",
        "            \"filename\": filename,\n",
        "            \"whisper_transcription\": result[\"text\"]\n",
        "        })\n",
        "\n",
        "# Create DataFrame\n",
        "df_transcripts = pd.DataFrame(transcripts)\n",
        "\n",
        "# Optional: Show or save\n",
        "print(df_transcripts)\n",
        "df_transcripts.to_csv(\"/content/transcriptions.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "_4e3EfTeLYXY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "DdOsEDOmS2GP",
        "zRRWrFkuXuPL",
        "I9_UG5OJS-1W",
        "uGA36eK2TENY",
        "BNHl_kMrUxZL"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}